# Decoders
This repository explains how a decoder-only transformer work. This explanation is based on the material provided by  Prof. Massimo Piccardi at the UTS 

## Agenda

1. **Overview of Decoder-Only Transformer Architecture**
    - Definition and primary function of a decoder in transforming input tokens into probability vectors.
    - Impact of vocabulary size on the dimensions of the output vectors.

2. **Input-Output Relationship in Decoders**
    - Detailed explanation of the input tokens (x1, x2, ..., xT) and the structure of output probability vectors (p1, p2, ..., pT).
    - The comprehensive size of probability vectors corresponding to each input token.

3. **Training Methodology and Objectives**
    - Strategies for maximizing the likelihood of target tokens (y1, y2, ..., yT).
    - Theoretical justification for maximizing the logarithm of probabilities.
    - Transforming the maximization of log probabilities into a minimization problem.

4. **Runtime Dynamics of Token Prediction**
    - Description of the sequential prediction of response tokens.
    - The cyclical nature of using predicted tokens for further predictions.

5. **Optimization of Decoder Parameters during Training**
    - Supervised training methods for fine-tuning the decoder's parameters (theta).
    - Techniques for aligning the predicted sequence with the target sequence, focusing on minimizing the negative log likelihood.

6. **Distinctive Features of Decoder-Only Architecture**
    - Comparative analysis with traditional transformer models, highlighting the simplifications in decoder-only architecture.
    - Discussion on the advantages of these simplifications and their possible reasons for inclusion in models like GPT.

7. **Conclusion**
    - Recap of the decoder-only transformer's ability to generate textual sequences.
    - The significance of the training phase in reducing the discrepancy between predicted and target sequences.

1. **Overview of Decoder-Only Transformer Architecture**
I will try to explain how a decoder-only transformer works. The decoder can be described as a module that receives in input, a sequence of tokens that we can note, as say, x1, x2, generic xt, down to the last token, xT. In output, a decoder produces a corresponding sequence of probability vectors. So we could note that, for instance, is p1, p2, pt all the way down to pT. These probability vectors have the size of the vocabulary that we are using. So say we are using a vocabulary with 50,000 words, each of these probability vectors demand, for instance, is 50,000 that's in size. So it's a huge with the same number is entries as the vocabulary, with a size equal to the same number of entries in the vocabulary, that being probabilities, all these numbers are between 0 and 1. and the sum of the numbers in each of these probability vectors is equal to what this is standard probability with a category comparative. So that's exactly what the decoder does. 
 
 
 
 Let us say that we want to train paste. The Coder penny means that we want to find the value optimal value for all its internal parameters, that we collectively note as theta. all of them. No, there's so many layers, parameters, weights, let's not. And collectively, just as feet up want to find the optimal value so that the decoder performs optimally in some sense. Now so typically for training in a supervised manner, we are given a sequence of ground truth tokens that become the target of the train. These Grant tools tokens. We can note them. Say, yes, y, one. y 2 YT all the way down to y big B. So where is the vocabulary? Has 50,000 tokens in each slot? Here we have one that we set to be the target of the train.

 What does it mean to target? That is, we simply want that the probability. in the probability vector for that particular token given to us as the target be as high as possible as high as possible means at best one worse could be 0 point 9 or something depending on the trade off, because we're not given just a single sequence as target. We're given many sequences, many samples, many pairs of input tokens and output targets. So the system will compromise. We're trying to find the value of theta that can find the best trade-off to give as high a probability as possible to each of the tokens in the target for all the samples in the training set.

 Now. So for this particular sequence, what is the target? A bit more formally? So, as we said, we want to maximize the probability of y one of y 2 of YT and y be older target. So typically, as we already know, instead of maximizing the probability we maximize its logarithm. Why not directly the probability? Because this range between 0 and one is a bit difficult to optimize as a constrained optimization to respect these boundaries for the values that are legit for the probabilities. A number of other reasons for reasons of avoiding overflow, and so forth. There's a number of reasons why, instead of maximizing the probabilities, we maximize the log of the probabilities. But, importantly, is that because the log is a monotonically increasing function of its argument. Just to be clear, the parameters, theta that maximize the probabilities, and the parameters, theta that maximize the log of the probabilities are exactly the same. So we don't change our goal. The goal remains the same, maximizing the probabilities. But we do that by maximizing the logarithm.

 Since we have multiple tokens. We can write down that objective as maximizing the sum of all this token, say from one to big tea of the log. of the probability of YT. Given of course, all the input X 1, 2 big P, and whatever parameters we have set. so given the tokens in, input. we want to maximize the probability of each. the log probability of each of the tokens in output. Just since we have t of them there's some this is the goal of the training the training ones to maximize this. but because conventionally. all the training objectives in packages are framed as loss functions and by convention, instead of maximizing, we want to minimize, then we simply add a minus sign in front. so that instead of finding the maximum possible value by playing with theta. For this log probability, we play with it in the same way, but we minimize the minus sum, which is exactly the opposite, like a function that instead of being shaped like this to find the maximum we boom, we flip it, we rotate it, and we want to soap. we, we, we, we reflect it sorry. And we want to find the minimum. Okay? So just a change inside to frame this objective and sustain that conventional way of minimization. This is the negative log likelihood or cross entropy. People call it interchangeably.

 So. This is the goal of training. We are given the sequence of tokens in input given a sequence of target tokens, corresponding target tokens in output. The goal of training is to minimize the negative log probability of the target tokens in output from the decoder. Now. when can this arrangement be useful? Let's make a bit of room here.

 The typical situation in which this module is useful is for question, answering or prompt, prompt response kind of problems in those problems. The the Coder at Runtime works in the following way. we provide a set of inputs to a certain point. Let's say, for instance, we provide the first small T tokens, as input in correspondence of these slots, the coda provides the probabilities that we described before, but we don't care about them. So we're not interested in what is the output from these other than the last token here.

 Xt. Well, we are interested in predicting the first token of a response. Let's say y one. Then we take y one and we put y. One as input input this time as the next token, after the last token of the prompt, and we hope to predict y to the second token of the response. Then we put Y 2 in input again, and we hope to predict y 3, and so forth and so forth and so forth. So we only provide the first t tokens as prompt or prefix or input. and we hope to obtain to follow the prediction of all the other tokens which form the response. And that is done, as you see, sequentially in the sense that while the prompt can be provided as a whole, because all these tokens are available at once. The prediction of the tokens of the response has to be sequential because one token is needed to become the input for the prediction of the next.

 So this is what happens at runtime or inference time. So how should we train in practice? One of these decoders to prepare it for through the training stage. What will be the negative log likelihood to actually use? Not to prepare the system for a prediction like this? So it's quite simple. Let's say that we, with an

 example, we are given a sentence like. How are you? And the response has to be. I'm well. So these, however, you are the tokens in input and the predictions are. I am well. So somewhere here. The negative of likelihood fundamentally needs to predict these tokens. using these tokens as the input at training time, all tokens are available. All those of the prompt and all those of the manually annotated response. So we can easily provide as input x one x 2 x 3 y. One and y 2, expecting in correspondence where we see those slots to predict y, one, y, 2 and y 3. So what we need to do is very minimal. we fundamentally don't need any target here for the negative log likelihood. So all that the system has to do is to dismiss in the negative log likelihood these tokens and praying for these tokens how the Gp library seems to be doing these is by providing the entire sequence as input to the recorder, input not necessarily as input physically, potentially, yes, because we can predict that end of sentence token as well at the end. to be aware of the length of the entire sequence. and to be aware of the length of the prefix.

 Then, when you create the negative log likelihood, you pass the entire sentence to it. the prompt or prefix and the response. But you also tell the system that it should mask length of token. which is somehow related or implied by the length of the prefix, so could be t or t minus one tokens whatever is not needed in this sentence and make the actual sentence the prompt sorry, the response only the target of the negative log. So this reconstruction function or loss function that we've seen repeatedly ticks fundamentally in input 3 arguments or, say 4. The first is the entire sequence which is the prompt in the response. the second is the length of the sequence. the third is the length of the prefix alone, so you know where to stop with the prompt, and where to instead account for what tokens account for in the negative log likelihood. And there seems to be a fourth argument that is called mask prefix false or or true by default. Mask prefix is true, so it means that these tokens are masked. but potentially one could also train the system to predict these tokens. So that sorry x 2, not y, 2 x 2 and x 3. So given any input x one. one may want to predict x. 2. Why would it doesn't harm to give a sensible objective also to the prompt itself. Perhaps it can help by setting somehow the internal states in this lot in a bit more useful manner is is not necessarily obvious. X, 2 is the same. X. 2 could be used to self, predict x. 3, as the next token in the prompt itself. and then eventually the last token of the prompt, as we said, has to predict the first token of the response exactly like before. So potentially the negative low likelihood could also have. This argument must prefix set to false. so that both the tokens of of the prefix and the token of the tokens of the response become the target of the training of the model a priori. I do not see really a clear reason why one should work better than the other. but perhaps empirically. Typically one works better and the other, and people tend to use it. I'm just not sure a possible small variation, tiny variation of the scheme could be that one could provide also beginning of sent and special token as the very first token, and also predict X one itself the first token of the prompt notional amendments. I'm not technically sure whether a beginning of sentence token is used in a Gp. or if the first token, the actual to first actual token of the prompt goes in as the first token, it really is this, this, somebody issues around these around the main transformers like D, 5, and but that gpt, so this, these are small notional changes. The fundamentals of what we say. always pretty much true.

 So that's how the the coda only architecture works. We could briefly contrast this, or how a normal transformer works a transformer made of encoder and the decoder both modules. Let me briefly say that so an encoder decoder architecture is not that different? It's just that those modules that we just described before you got 2. One is called the encoder. One is called decoder. In the the separation between the prompt and the response is physical in this case, because the prompt goes in input to the encoder and the response is output by the decoder. So this clear separation of modules for the input and for the output in our example, just to briefly say we would have the same bego beginning of sentence, sorry that we will have the first token, we don't need this. I'm sorry. Probably here, deleted. we would have say we we could have it as well again, then no reason potentially. Why not. But let's say that directly we have x one xt in between all the tokens of the input. the encoder works on a fixed length. so there will be padding tokens if the if the actual prompt doesn't exhaust the length available fixed length available for the encoder. there will be just padding special tokens to pad it up maximum length from each of these. We don't need any prediction, so the encoder doesn't have in this case any kind of probability outputs the probability outputs are again out of the decoder e. 1, 2, say d. we should use a difference. Ap, n, because the in this case the length of the again. We don't know how long will be. The response can be obviously a different length from the prompt. So if we said xt for the prompt alone. In this case, big T. If we use the same notation, we should use the small T here. Sorry just in continuation. But just to be clear. There's a clear separation. So we use x. One to xt to denote the prompt, and p. One to PN. To the note. The outputs of the tokens, and the numbers of them different like before.

 So how the 2 modules are connected. The 2 models are connected through a huge number of connections which collectively are called the cross attention. So the tokens of the input get all massage and manipulated into internal states. The internal States are communicated to the decoder through this cross attention mechanism. Then we provide a beginning of sentence token here to the the coder as the first token in, input and then we get the probability of the first output broken in output. Then we. if we are training, we will have a reference y. One YF. If we use it at inference, we predict instead a token from p. One. typically that R. Max, the token with the largest value in the probability vector we put that in input here. we predict why 2, we put y 2 in the input or the next prediction, and so forth and so forth, exactly like before.

So at inference time, like before, we predict one token at a time. that token goes in input to the next lot and produces the next token as the prediction, and so forth, and so forth, and so forth. To be simple. This is called the greedy, the coding. You can do that a bit more smartly by accumulating, not just one prediction, but 5 or 6 or 10, and that's called beam decoding beam search, but doesn't matter.

Now, what's important at training time, as before, all these tokens are available, the input tokens and the tokens of the targeted sentence or ground truth sentence. So, at training time, the arrangement you see here is exactly how the model is trained by providing all the prompt tokens to the encoder but providing all the response tokens as inputs to the decoder shifted by one position and then by maximizing the log probability of the target tokens or minimizing the negative log probability, so minimizing the negative log likelihood.

So the two architectures are really, very, very similar. The main difference is that this is the traditional transformer. The decoder-only architecture is a bit of a simplification because fundamentally it gets rid of all these padding tokens to bring up the input to a maximum length. As soon as the input is finished, we start the prediction of the first token of the response, and there's no need for cross attention because it's a single module, and pretty much the self-attention within that module does what the cross-attention does between the encoder and decoder. So all predictions are informed by the hidden state of every other token available in the sentence.

So there's no significant difference in the decoder-only architecture compared to an encoded-decoded transformer. Perhaps it's a bit simpler. Perhaps that's why people in OpenAI decided to make GPT a decoder-only architecture. The important thing is, we understood what the training objective is and how the architecture works at runtime.


